{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d3815ec-3c31-4862-8408-4fc8854e9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d722733c-2ec7-4eb2-ab0b-f42bdebf20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(corpus):\n",
    "    corpus = [re.sub(r'https?://\\S+|www\\.\\S+', '', doc) for doc in corpus]\n",
    "    print(\"Removing URLs: \", corpus, \"\\n\")\n",
    "\n",
    "    # Removing special characters\n",
    "    corpus = [re.sub(r'[^a-zA-Z\\s]', '', doc) for doc in corpus]\n",
    "    print(\"Removing special characters: \", corpus, \"\\n\")\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "    print(\"Tokenized text: \", tokenized_corpus, \"\\n\")\n",
    "\n",
    "    # Checking pos tags for each word\n",
    "    pos_tagged_corpus = [pos_tag(doc) for doc in tokenized_corpus]\n",
    "    print(\"POS tagging: \", pos_tagged_corpus, \"\\n\")\n",
    "\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_corpus = [[word for word in doc if word not in stop_words] for doc in tokenized_corpus]\n",
    "    print(\"After stop word removal: \", tokenized_corpus, \"\\n\")\n",
    "\n",
    "    # Stemming and Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_lemmatized_corpus = [[lemmatizer.lemmatize(stemmer.stem(word)) for word in doc] for doc in tokenized_corpus]\n",
    "\n",
    "    return stemmed_lemmatized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79dea82d-273e-4f19-abd9-ecfb78982072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing URLs:  ['Hello World! üåç Welcome to NLP. Visit  for more info.', 'Python is amazing!!! @user #MachineLearning', 'Email me at test@example.com üòä or visit ', 'Numbers 123456 and special characters $#@!* should be removed.', 'Follow the updates at  and stay informed!', 'Breaking news: AI is revolutionizing industries! Read more at ', 'Contact support at support@helpdesk.com if you need assistance! üì©'] \n",
      "\n",
      "Removing special characters:  ['Hello World  Welcome to NLP Visit  for more info', 'Python is amazing user MachineLearning', 'Email me at testexamplecom  or visit ', 'Numbers  and special characters  should be removed', 'Follow the updates at  and stay informed', 'Breaking news AI is revolutionizing industries Read more at ', 'Contact support at supporthelpdeskcom if you need assistance '] \n",
      "\n",
      "Tokenized text:  [['hello', 'world', 'welcome', 'to', 'nlp', 'visit', 'for', 'more', 'info'], ['python', 'is', 'amazing', 'user', 'machinelearning'], ['email', 'me', 'at', 'testexamplecom', 'or', 'visit'], ['numbers', 'and', 'special', 'characters', 'should', 'be', 'removed'], ['follow', 'the', 'updates', 'at', 'and', 'stay', 'informed'], ['breaking', 'news', 'ai', 'is', 'revolutionizing', 'industries', 'read', 'more', 'at'], ['contact', 'support', 'at', 'supporthelpdeskcom', 'if', 'you', 'need', 'assistance']] \n",
      "\n",
      "POS tagging:  [[('hello', 'JJ'), ('world', 'NN'), ('welcome', 'NN'), ('to', 'TO'), ('nlp', 'VB'), ('visit', 'NN'), ('for', 'IN'), ('more', 'JJR'), ('info', 'NN')], [('python', 'NN'), ('is', 'VBZ'), ('amazing', 'VBG'), ('user', 'RBR'), ('machinelearning', 'NN')], [('email', 'VB'), ('me', 'PRP'), ('at', 'IN'), ('testexamplecom', 'NN'), ('or', 'CC'), ('visit', 'NN')], [('numbers', 'NNS'), ('and', 'CC'), ('special', 'JJ'), ('characters', 'NNS'), ('should', 'MD'), ('be', 'VB'), ('removed', 'VBN')], [('follow', 'VB'), ('the', 'DT'), ('updates', 'NNS'), ('at', 'IN'), ('and', 'CC'), ('stay', 'VB'), ('informed', 'JJ')], [('breaking', 'VBG'), ('news', 'NN'), ('ai', 'NN'), ('is', 'VBZ'), ('revolutionizing', 'VBG'), ('industries', 'NNS'), ('read', 'VBD'), ('more', 'RBR'), ('at', 'IN')], [('contact', 'JJ'), ('support', 'NN'), ('at', 'IN'), ('supporthelpdeskcom', 'NN'), ('if', 'IN'), ('you', 'PRP'), ('need', 'VBP'), ('assistance', 'VB')]] \n",
      "\n",
      "After stop word removal:  [['hello', 'world', 'welcome', 'nlp', 'visit', 'info'], ['python', 'amazing', 'user', 'machinelearning'], ['email', 'testexamplecom', 'visit'], ['numbers', 'special', 'characters', 'removed'], ['follow', 'updates', 'stay', 'informed'], ['breaking', 'news', 'ai', 'revolutionizing', 'industries', 'read'], ['contact', 'support', 'supporthelpdeskcom', 'need', 'assistance']] \n",
      "\n",
      "Stemmed and Lemmatized Text: [['hello', 'world', 'welcom', 'nlp', 'visit', 'info'], ['python', 'amaz', 'user', 'machinelearn'], ['email', 'testexamplecom', 'visit'], ['number', 'special', 'charact', 'remov'], ['follow', 'updat', 'stay', 'inform'], ['break', 'news', 'ai', 'revolution', 'industri', 'read'], ['contact', 'support', 'supporthelpdeskcom', 'need', 'assist']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Hello World! üåç Welcome to NLP. Visit https://example.com for more info.\",\n",
    "    \"Python is amazing!!! @user #MachineLearning\",\n",
    "    \"Email me at test@example.com üòä or visit www.website.org.\",\n",
    "    \"Numbers 123456 and special characters $#@!* should be removed.\",\n",
    "    \"Follow the updates at https://newsportal.com/latest-news and stay informed!\",\n",
    "    \"Breaking news: AI is revolutionizing industries! Read more at www.technews.com.\",\n",
    "    \"Contact support at support@helpdesk.com if you need assistance! üì©\"\n",
    "]\n",
    "\n",
    "processed_corpus = preprocess_text(corpus)\n",
    "print(\"Stemmed and Lemmatized Text:\", processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1239a02e-36b1-4539-92dc-f20a2cea6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: Counter({'visit': 2, 'hello': 1, 'world': 1, 'welcom': 1, 'nlp': 1, 'info': 1, 'python': 1, 'amaz': 1, 'user': 1, 'machinelearn': 1, 'email': 1, 'testexamplecom': 1, 'number': 1, 'special': 1, 'charact': 1, 'remov': 1, 'follow': 1, 'updat': 1, 'stay': 1, 'inform': 1, 'break': 1, 'news': 1, 'ai': 1, 'revolution': 1, 'industri': 1, 'read': 1, 'contact': 1, 'support': 1, 'supporthelpdeskcom': 1, 'need': 1, 'assist': 1})\n",
      "Number of words: 32\n",
      "Number of documents: 7\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists\n",
    "all_words = [word for doc in processed_corpus for word in doc]\n",
    "\n",
    "# Vocabulary and counts\n",
    "vocab = Counter(all_words)\n",
    "\n",
    "# Number of words\n",
    "num_words = len(all_words)\n",
    "\n",
    "# Number of documents\n",
    "num_documents = len(processed_corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Number of words:\", num_words)\n",
    "print(\"Number of documents:\", num_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9c04c-17c4-4d1d-93e4-896904f54dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f345b-e892-4ace-8fb6-6257f408c1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ee3e8-00af-41c5-b1ad-108d65d6a87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
