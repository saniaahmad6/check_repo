{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d77lYOV_ev0O",
    "outputId": "39af09f3-8016-4208-8657-0a54e656cbdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.10.1\n",
      "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
      "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
      "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
      "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
      "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
      "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3 scipy-1.10.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "1cc0af2b5e244ed7adb613a5c511dfdd",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'this' is in the model vocabulary.\n",
      "Vector for 'this':\n",
      " [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "\n",
      "Most similar words to 'this':\n",
      "a: 0.17018885910511017\n",
      "another: -0.013514922931790352\n",
      "\n",
      "Result of 'this - is + another': [('a', 0.07128647714853287)]\n"
     ]
    }
   ],
   "source": [
    "# Install specific versions of scipy and numpy known to be compatible with gensim\n",
    "# You might need to experiment with different versions if this doesn't work.\n",
    "# This combination has been found to work in some environments.\n",
    "!pip install scipy==1.10.1 numpy==1.24.3 --force-reinstall\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors # Import KeyedVectors explicitly\n",
    "\n",
    "# Load pre-trained Word2Vec model (Google News)\n",
    "# Download the Google News vectors from: https://code.google.com/archive/p/word2vec/\n",
    "# The file is large (~1.5GB), and the path to the file should be updated below.\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Make sure the GoogleNews-vectors-negative300.bin file is in the correct path\n",
    "# If you haven't downloaded it, you need to do so from the link above.\n",
    "# This line will load the model:\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# For demonstration purposes without the large file, we will create a dummy model:\n",
    "# If you have the file, comment out the following two lines and uncomment the line above.\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"this\", \"is\", \"a\", \"sentence\"], [\"this\", \"is\", \"another\", \"sentence\"]]\n",
    "word2vec_model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "\n",
    "# Check if a word is in the vocabulary\n",
    "word = \"this\" # Use a word from the dummy vocabulary\n",
    "if word in word2vec_model.wv: # Access the KeyedVectors object via .wv\n",
    "    print(f\"'{word}' is in the model vocabulary.\")\n",
    "\n",
    "# Get the vector for a specific word\n",
    "vector = word2vec_model.wv[word] # Access the vector via .wv\n",
    "print(f\"Vector for '{word}':\\n\", vector)\n",
    "\n",
    "# Find the most similar words to a given word\n",
    "# Use a word from the dummy vocabulary\n",
    "similar_words = word2vec_model.wv.most_similar(\"this\", topn=2) # Access most_similar via .wv\n",
    "print(\"\\nMost similar words to 'this':\")\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity}\")\n",
    "\n",
    "# Perform vector arithmetic: king - man + woman = ?\n",
    "# This operation is more meaningful with a larger, trained model.\n",
    "# Using the dummy model might not produce expected linguistic results.\n",
    "# If you are using the dummy model, adjust the positive/negative words to be in the vocabulary.\n",
    "try:\n",
    "    result = word2vec_model.wv.most_similar(positive=[\"this\", \"another\"], negative=[\"is\"], topn=1)\n",
    "    print(\"\\nResult of 'this - is + another':\", result)\n",
    "except KeyError as e:\n",
    "    print(f\"\\nCould not perform vector arithmetic. Ensure all words are in the vocabulary. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-f9BXdMMfXKx",
    "outputId": "425aab93-e8c8-454b-8618-d39c093a826d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
      " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
      "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
      " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
      "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
      "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
      "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
      "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
      "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
      "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
      "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
      "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
      " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
      " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
      " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
      "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
      "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
      " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
      " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
      " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
      "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
      "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
      "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
      " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
      " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
      "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
      " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
      "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
      " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
      " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
      "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
      " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
      " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
      "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
      " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
      "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
      "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
      " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
      " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
      " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
      " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
      " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
      "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
      "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
      "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
      " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
      "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
      "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
      " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
      " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
      " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
      "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
      "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
      " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
      "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
      " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
      "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
      "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
      " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
      "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
      "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
      "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
      "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
      "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
      " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
      "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
      " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
      "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
      "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
      " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
      "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
      "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
      "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
      " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
      " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n",
      "[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422105193138123)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Example usage\n",
    "print(model['king'])  # vector for the word 'king'\n",
    "print(model.most_similar('king'))  # similar words to 'king'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxLGnzQifXW7",
    "outputId": "4f29707f-8e37-45cb-a363-8988c4dfb6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.13227147e-03 -4.45733406e-03 -1.06835726e-03  1.00636482e-03\n",
      " -1.91113955e-04  1.14817743e-03  6.11386076e-03 -2.02715401e-05\n",
      " -3.24596534e-03 -1.51072862e-03  5.89729892e-03  1.51410222e-03\n",
      " -7.24261976e-04  9.33324732e-03 -4.92128357e-03 -8.38409644e-04\n",
      "  9.17541143e-03  6.74942741e-03  1.50285603e-03 -8.88256077e-03\n",
      "  1.14874600e-03 -2.28825561e-03  9.36823711e-03  1.20992784e-03\n",
      "  1.49006362e-03  2.40640994e-03 -1.83600665e-03 -4.99963388e-03\n",
      "  2.32429506e-04 -2.01418041e-03  6.60093315e-03  8.94012302e-03\n",
      " -6.74754381e-04  2.97701475e-03 -6.10765442e-03  1.69932481e-03\n",
      " -6.92623248e-03 -8.69402662e-03 -5.90020278e-03 -8.95647518e-03\n",
      "  7.27759488e-03 -5.77203138e-03  8.27635173e-03 -7.24354526e-03\n",
      "  3.42167495e-03  9.67499893e-03 -7.78544787e-03 -9.94505733e-03\n",
      " -4.32914635e-03 -2.68313056e-03 -2.71289347e-04 -8.83155130e-03\n",
      " -8.61755759e-03  2.80021061e-03 -8.20640661e-03 -9.06933658e-03\n",
      " -2.34046578e-03 -8.63180775e-03 -7.05664977e-03 -8.40115082e-03\n",
      " -3.01328895e-04 -4.56429832e-03  6.62717456e-03  1.52716041e-03\n",
      " -3.34147573e-03  6.10897178e-03 -6.01328490e-03 -4.65616956e-03\n",
      " -7.20750913e-03 -4.33658017e-03 -1.80932996e-03  6.48964290e-03\n",
      " -2.77039292e-03  4.91896737e-03  6.90444233e-03 -7.46370573e-03\n",
      "  4.56485013e-03  6.12697843e-03 -2.95447465e-03  6.62502181e-03\n",
      "  6.12587947e-03 -6.44348515e-03 -6.76455162e-03  2.53895880e-03\n",
      " -1.62381888e-03 -6.06512791e-03  9.49920900e-03 -5.13014663e-03\n",
      " -6.55409694e-03 -1.19885204e-04 -2.70142802e-03  4.44400299e-04\n",
      " -3.53745813e-03 -4.19330609e-04 -7.08615757e-04  8.22820642e-04\n",
      "  8.19481723e-03 -5.73670724e-03 -1.65952800e-03  5.57160750e-03]\n",
      "[('to', 0.1459505707025528), ('fun', 0.041577354073524475), ('word2vec', 0.03476494178175926), ('sentence', 0.01915225386619568), ('is', 0.01613469421863556), ('this', 0.008826184086501598), ('a', 0.004842504393309355), ('learn', -0.11410722881555557)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\"This is a sample sentence\", \"Word2Vec is fun to learn\"]\n",
    "tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save and load\n",
    "model.save(\"custom_word2vec.model\")\n",
    "model = Word2Vec.load(\"custom_word2vec.model\")\n",
    "\n",
    "print(model.wv['sample'])\n",
    "print(model.wv.most_similar('sample'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7MxdLt4ewjK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
